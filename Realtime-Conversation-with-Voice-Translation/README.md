# ğŸŒ Real-time Conversation with Voice Translation

A machine learning project that facilitates **real-time conversation between an English-speaking person and a Spanish-speaking person** using voice input, neural machine translation, and text-to-speech output.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Flask](https://img.shields.io/badge/Flask-2.3+-green.svg)
![License](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ“‹ Project Description

This project implements a complete voice translation system that:

- **Extracts Spanish words from voice input** and translates them into English, then reads the translated text aloud
- **Takes English voice input** from the other user, translates it into Spanish, and reads the translated text aloud
- Uses a **custom-built machine learning model** (Seq2Seq with Attention mechanism)
- Provides both **web interface** and **command-line interface**

## âœ¨ Features

| Feature                | Description                                            |
| ---------------------- | ------------------------------------------------------ |
| ğŸ¤ **Voice Input**     | Real-time speech recognition for English and Spanish   |
| ğŸ¤– **Custom ML Model** | Seq2Seq with Bahdanau Attention (trained from scratch) |
| ğŸ”Š **Voice Output**    | Text-to-speech for translated text                     |
| ğŸŒ **Web Interface**   | Beautiful, responsive web UI                           |
| âš¡ **Real-time**       | Instant translation with low latency                   |
| ğŸ”„ **Bidirectional**   | English â†” Spanish translation                          |

## ğŸ—ï¸ Project Structure

```
Realtime-Conversation-with-Voice-Translation/
â”œâ”€â”€ app.py                      # Flask web application
â”œâ”€â”€ main.py                     # Command-line interface
â”œâ”€â”€ voice_conversation.py       # Voice conversation system
â”œâ”€â”€ Project_Analysis.ipynb      # Jupyter notebook with visualizations
â”œâ”€â”€ requirements.txt            # Project dependencies
â”œâ”€â”€ test_setup.py              # Installation verification
â”œâ”€â”€ view_progress.py           # Training progress monitor
â”‚
â”œâ”€â”€ models/                    # Neural network models
â”‚   â”œâ”€â”€ seq2seq.py            # Seq2Seq with Attention architecture
â”‚   â””â”€â”€ model_utils.py        # Model utilities
â”‚
â”œâ”€â”€ training/                  # Training scripts
â”‚   â”œâ”€â”€ train_translator.py   # Main training script
â”‚   â”œâ”€â”€ evaluate.py           # Model evaluation
â”‚   â””â”€â”€ config.py             # Training configuration
â”‚
â”œâ”€â”€ src/                       # Source modules
â”‚   â”œâ”€â”€ translator.py         # Custom model translator
â”‚   â”œâ”€â”€ translator_pretrained.py  # Pre-trained model translator
â”‚   â”œâ”€â”€ speech_recognition_module.py  # Voice to text
â”‚   â”œâ”€â”€ text_to_speech.py     # Text to voice
â”‚   â””â”€â”€ conversation.py       # Conversation flow
â”‚
â”œâ”€â”€ data/                      # Data processing
â”‚   â””â”€â”€ data_loader.py        # Dataset loading from Hugging Face
â”‚
â”œâ”€â”€ templates/                 # Web interface
â”‚   â””â”€â”€ index.html            # Main web page
â”‚
â””â”€â”€ checkpoints/              # Trained models
    â”œâ”€â”€ en_es/                # English â†’ Spanish model
    â””â”€â”€ es_en/                # Spanish â†’ English model
```

## ğŸ”§ Installation

### Prerequisites

- Python 3.8 or higher
- CUDA-compatible GPU (optional, for faster training)
- Microphone (for voice input)
- Speakers (for voice output)

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/Realtime-Conversation-with-Voice-Translation.git
cd Realtime-Conversation-with-Voice-Translation
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

**Windows Users - PyAudio Installation:**
If PyAudio fails to install, download the wheel file from [here](https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyaudio) and install:

```bash
pip install PyAudio-0.2.11-cp311-cp311-win_amd64.whl
```

### Step 3: Verify Installation

```bash
python test_setup.py
```

## ğŸš€ Quick Start

### Option 1: Web Interface (Recommended)

```bash
python app.py
```

Open your browser and go to: **http://127.0.0.1:5000**

### Option 2: Command Line Interface

```bash
python main.py
```

### Option 3: Voice Conversation Mode

```bash
python voice_conversation.py
```

## ğŸ§  Model Architecture

### Custom Seq2Seq with Bahdanau Attention

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENCODER (Bidirectional LSTM)              â”‚
â”‚  Input: "Hello how are you"                                  â”‚
â”‚  â†“                                                           â”‚
â”‚  Embedding Layer (256 dim) â†’ BiLSTM (512 hidden Ã— 2 layers) â”‚
â”‚  â†“                                                           â”‚
â”‚  Encoder Outputs + Hidden States                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ATTENTION MECHANISM                       â”‚
â”‚  Bahdanau (Additive) Attention                              â”‚
â”‚  - Computes alignment scores                                 â”‚
â”‚  - Creates context vector from encoder outputs               â”‚
â”‚  - Focuses on relevant source words for each target word    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DECODER (LSTM)                            â”‚
â”‚  Context Vector + Previous Token â†’ LSTM (512 hidden)        â”‚
â”‚  â†“                                                           â”‚
â”‚  Output: "Hola cÃ³mo estÃ¡s"                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Specifications

| Component               | Specification                      |
| ----------------------- | ---------------------------------- |
| **Encoder**             | 2-layer Bidirectional LSTM         |
| **Decoder**             | 2-layer LSTM with Attention        |
| **Embedding Dimension** | 256                                |
| **Hidden Dimension**    | 512                                |
| **Vocabulary Size**     | 15,000 words                       |
| **Total Parameters**    | ~16 million                        |
| **Training Data**       | 50,000 sentence pairs (opus_books) |
| **Dropout**             | 0.3                                |

## ğŸ“Š Model Performance

### Training Results

| Model       | Best Epoch | Training Loss | Validation Loss | Perplexity |
| ----------- | ---------- | ------------- | --------------- | ---------- |
| **EN â†’ ES** | 21         | 4.53          | 5.93            | ~375       |
| **ES â†’ EN** | 22         | 4.54          | 6.34            | ~566       |

### Training Details

- **Dataset**: opus_books (English-Spanish parallel corpus)
- **Training Samples**: 40,000 pairs
- **Validation Samples**: 5,000 pairs
- **Test Samples**: 5,000 pairs
- **Epochs**: 31 (with early stopping, patience=10)
- **Batch Size**: 128
- **Optimizer**: Adam (lr=0.001)
- **Device**: NVIDIA GeForce GTX 1650 (CUDA)

### ğŸ“ˆ Visualizations

For detailed training curves, model comparison, and analysis, see the **[Project_Analysis.ipynb](Project_Analysis.ipynb)** notebook which includes:

- Training and validation loss curves
- Model architecture visualization
- Custom model vs Pre-trained model comparison
- Translation examples
- System pipeline diagram

## ğŸ¯ How It Works

### 1. Voice Input (Speech Recognition)

```python
# Using Google Speech Recognition API
recognizer = SpeechRecognizer(language='en-US')
text = recognizer.listen_from_microphone()
# â†’ "Hello, how are you?"
```

### 2. Translation (Neural Machine Translation)

```python
# Using custom Seq2Seq or pre-trained Helsinki-NLP model
translator = PretrainedTranslator()
translation = translator.translate("Hello, how are you?", "en-es")
# â†’ "Hola, Â¿cÃ³mo estÃ¡s?"
```

### 3. Voice Output (Text-to-Speech)

```python
# Using pyttsx3 or gTTS
tts = TextToSpeech(language='es')
tts.speak("Hola, Â¿cÃ³mo estÃ¡s?")
# â†’ Audio plays: "Hola, Â¿cÃ³mo estÃ¡s?"
```

## ğŸ› ï¸ Training Your Own Model

### Train English â†’ Spanish Model

```bash
python training/train_translator.py --direction en-es
```

### Train Spanish â†’ English Model

```bash
python training/train_translator.py --direction es-en
```

### Monitor Training Progress

```bash
python view_progress.py
```

### Evaluate Model

```bash
python training/evaluate.py --direction en-es --interactive
```

### Configuration

Edit `training/config.py` to adjust:

- `BATCH_SIZE`: Reduce if running out of memory
- `NUM_EPOCHS`: Increase for better quality
- `MAX_VOCAB_SIZE`: Vocabulary limit
- `HIDDEN_DIM`: Model capacity

## ğŸŒ Web Interface

The web interface provides:

- ğŸ¤ **Voice Input Button**: Click to speak
- ğŸ”„ **Language Switch**: Toggle between ENâ†’ES and ESâ†’EN
- âœ¨ **Translate Button**: Manual translation trigger
- ğŸ”Š **Listen Button**: Hear the translation spoken aloud
- ğŸ“‹ **Copy Button**: Copy translation to clipboard

### Screenshots

The interface features:

- Dark gradient theme
- Real-time voice wave animations
- Responsive design for mobile and desktop
- Instant translation feedback

## ğŸ“ Files Description

| File                               | Purpose                                |
| ---------------------------------- | -------------------------------------- |
| `app.py`                           | Flask web server with translation API  |
| `main.py`                          | Command-line entry point               |
| `voice_conversation.py`            | Full voice conversation system         |
| `models/seq2seq.py`                | Neural network architecture definition |
| `training/train_translator.py`     | Training script with checkpointing     |
| `src/translator_pretrained.py`     | Helsinki-NLP MarianMT integration      |
| `src/speech_recognition_module.py` | Microphone input handling              |
| `src/text_to_speech.py`            | Audio output generation                |

## ğŸ”Œ API Endpoints

### Translation Endpoint

```http
POST /translate
Content-Type: application/json

{
    "text": "Hello, how are you?",
    "direction": "en-es"
}
```

**Response:**

```json
{
  "translation": "Hola, Â¿cÃ³mo estÃ¡s?",
  "original": "Hello, how are you?",
  "direction": "en-es"
}
```

## ğŸ¤ Technologies Used

- **PyTorch**: Deep learning framework
- **Transformers**: Pre-trained models (Helsinki-NLP/MarianMT)
- **Flask**: Web framework
- **SpeechRecognition**: Voice input
- **pyttsx3 / gTTS**: Text-to-speech
- **Hugging Face Datasets**: Training data (opus_books)

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Hugging Face](https://huggingface.co/) for datasets and pre-trained models
- [Helsinki-NLP](https://huggingface.co/Helsinki-NLP) for MarianMT translation models
- [opus_books](https://opus.nlpl.eu/) for the parallel corpus dataset

## ğŸ‘¤ Author

**Shreyas** - Internship Project

---

â­ If you found this project helpful, please give it a star!
